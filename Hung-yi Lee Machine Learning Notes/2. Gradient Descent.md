## 2. Gradient Descent

使用梯度下降法来解决优化问题。梯度下降的方向是移动的反方向。

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.1.PNG)
---

### Tip1: Tuning your learning rates

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.2.PNG)

调整学习速率：学习速率是需要在训练过程中调整。

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.3.PNG)
---

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.4.PNG)

推导：

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.5.PNG)
---

### Tip2: Stochastic Gradient Descent 

目的是使训练更快。

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.6.PNG)

重要区别：

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.7.PNG)
---

### Tip3: Feature Scaling

Make different features have the same scaling

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.8.PNG)
---

## Gradient Descent Theory

泰勒序列：

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.9.PNG)

多个变量的泰勒序列:

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.10.PNG)

基于上面泰勒序列，表示 Loss 函数：

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.11.PNG)

最后推导：

![](https://github.com/steveLauwh/DeepLearning-notes/raw/master/Hung-yi%20Lee%20Machine%20Learning%20Notes/image/2.12.PNG)

